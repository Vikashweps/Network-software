# Быстрее, выше, сильнее?

"Сайт тормозит" — это жалоба пользователя. Инженер говорит цифрами.

## Основные метрики
1. **Latency (Задержка)**: Время от отправки запроса до получения ответа.
   - Важно смотреть не на среднее (Avg), а на **перцентили** (P50, P95, P99).
   - Если P99 = 2с, это значит, что у 1% пользователей (самых неудачливых) запрос висит 2 секунды.
2. **Throughput (Пропускная способность)**: Количество запросов в секунду (RPS - Requests Per Second).
3. **Saturation (Насыщение)**: Насколько загружена система (CPU, Memory, Network IO).

## Закон Литтла (Little's Law)
В стабильной системе: `Число пользователей в системе = RPS * Latency`.
Если ваш сервер держит 100 RPS и отвечает за 0.5с, значит, в любой момент времени он обрабатывает 50 параллельных запросов.

## gRPC vs REST
В теории gRPC быстрее. На практике — зависит от условий.
- На малых данных (получить ID) разница минимальна.
- На больших структурах Protobuf выигрывает у JSON по CPU (сериализация).
- На плохой сети HTTP/2 (gRPC) выигрывает за счет мультиплексирования.

## Инструменты
- **wrk**: Очень быстрый генератор HTTP нагрузки (на C).
- **ghz**: Аналог wrk для gRPC.
- **Locust**: Пишем сценарии на Python. Удобно, но сам Locust ест много CPU, сложно создать огромную нагрузку с одной машины.
